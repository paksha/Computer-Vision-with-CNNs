{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30673,
     "status": "ok",
     "timestamp": 1607485305596,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "ClrxdsCtDzJ5",
    "outputId": "e0f8cd63-36b1-4a8d-fd69-52fc613e1137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9475,
     "status": "ok",
     "timestamp": 1607485322662,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "gAPBH5hyGWlI",
    "outputId": "0df058c3-b957-485a-a240-9129325e5ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -q install git+https://github.com/qubvel/classification_models.git pickle5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run this Code\n",
    "This code can be run best on Google Colab, or on a sufficiently powerful machine. The paths used in this version are local paths of a directory where this notebook should be located. This directory should include the shuffled-images/ folder which includes all the images of the dataset, and it should also include the train.csv and test.csv files. It also needs a Data/ folder in the same directory to be created to which processed data files can be saved.\n",
    "\n",
    "If you are running this notebook on Colab, make sure to use the variable named \"path\" for your convenience after setting it to the right path. The models used have been downloaded (the ResNet18 model and the general model classifier) for reproducibility, but note that the exact same results might not be produced due to a variety of reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2012,
     "status": "ok",
     "timestamp": 1607485326043,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "pwY3w040Eh89"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16, InceptionResNetV2, MobileNetV2, Xception, InceptionV3, ResNet50\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, GRU\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from classification_models.keras import Classifiers\n",
    "from tqdm import tqdm # use tqdm from tqdm.notebook if on Colab\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "from collections import Counter\n",
    "\n",
    "act='relu'\n",
    "num_classes = 27\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "data_augmentation = True\n",
    "path = \"\" # '/content/gdrive/My Drive/Colab Notebooks/Kaggle/'\n",
    "\n",
    "classes = ['Accessories','Apparel Set','Bags','Belts','Bottomwear',\n",
    " 'Cufflinks','Dress','Eyewear','Flip Flops','Fragrance','Free Gifts',\n",
    " 'Headwear','Innerwear','Jewellery','Lips','Loungewear and Nightwear',\n",
    " 'Makeup','Nails','Sandal','Saree','Scarves','Shoes','Socks','Ties','Topwear',\n",
    " 'Wallets','Watches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1607485675838,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "5Gd-2_VLQ8IK"
   },
   "outputs": [],
   "source": [
    "# Code used to process data\n",
    "# Paths may be wrong as some of these functions were used on my local machine\n",
    "def processImages(inp=path + 'train.csv', out=path + 'Data/TRAIN_IMAGES.pkl'):\n",
    "    df = pd.read_csv(inp)\n",
    "    images = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        img = load_img('shuffled-images/{}.jpg'.format(str(row['id'])), grayscale=False)\n",
    "        images.append(img_to_array(img))\n",
    "    images = np.array(images)\n",
    "    print(images.shape)\n",
    "    with open(out, 'wb') as handle:\n",
    "      pickle.dump(images, handle)\n",
    "\n",
    "def trainLabelsOrder():\n",
    "    df = pd.read_csv(path + 'train.csv')\n",
    "    labels = []\n",
    "    for idx, row in df.iterrows():\n",
    "        labels.append(classes.index(row['category']))\n",
    "    labels = np.array(labels)\n",
    "    print(labels.shape)\n",
    "    with open(path + \"Data/TRAIN_LABELS.pkl\", 'wb') as handle:\n",
    "      pickle.dump(labels, handle)\n",
    "\n",
    "def cleanText(file='train.csv'):\n",
    "    def hasNumbers(s): return any(char.isdigit() for char in s)\n",
    "    df = pd.read_csv(file)\n",
    "    for idx, row in df.iterrows():\n",
    "        noisyTextTokens = row['noisyTextDescription'].split(' ')\n",
    "        cleanTokens = [n for n in noisyTextTokens if not hasNumbers(n)]\n",
    "        df.at[idx, 'noisyTextDescription'] = ' '.join(cleanTokens)\n",
    "    df.to_csv('clean_{}'.format(file))\n",
    "\n",
    "def processCleanText(file='clean_train.csv', out='Data/TRAIN_PLAINTEXT.pkl'):\n",
    "    df = pd.read_csv(file)\n",
    "    text = []\n",
    "    for idx, row in df.iterrows():\n",
    "        row_text = \"\"\n",
    "        row_text += row['gender']\n",
    "        row_text += \" \" + row['baseColour']\n",
    "        row_text += \" \" + row['season']\n",
    "        row_text += \" \" + row['usage']\n",
    "        if str(row['noisyTextDescription']) != \"nan\":\n",
    "            row_text += \" \" + str(row['noisyTextDescription'])\n",
    "        text.append(row_text)\n",
    "    with open(out, 'wb') as handle:\n",
    "      pickle.dump(text, handle)\n",
    "\n",
    "def loadTrainDataInOrder():\n",
    "  with open(path + 'Data/TRAIN_IMAGES.pkl', 'rb') as f: train_images = pickle.load(f)\n",
    "  with open(path + 'Data/TRAIN_PLAINTEXT.pkl', 'rb') as f: train_text = pickle.load(f)\n",
    "  with open(path + 'Data/TRAIN_LABELS.pkl', 'rb') as f: train_labels = pickle.load(f)\n",
    "  train_images = train_images.astype('float32')\n",
    "  return train_images/255., train_text, to_categorical(train_labels)\n",
    "\n",
    "def loadTestData():\n",
    "  # TEST_IMAGES.pkl stores the images in order of the sample submission IDs\n",
    "  with open(path + \"Data/TEST_IMAGES.pkl\", 'rb') as f:\n",
    "    test_images = pickle.load(f)\n",
    "  with open(path + \"Data/TEST_PLAINTEXT.pkl\", 'rb') as f:\n",
    "    test_text = pickle.load(f)\n",
    "  test_images = test_images.astype('float32')\n",
    "  return test_images/255, test_text\n",
    "\n",
    "def predToClass(p): return classes[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JjdihTgf07Zd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21627it [01:10, 308.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21627, 80, 60, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21628it [02:23, 150.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21628, 80, 60, 3)\n",
      "(21627,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Uncomment the next two lines to process image \n",
    "data into .pkl files which can be used in the models\n",
    "'''\n",
    "processImages()\n",
    "processImages(inp=path + 'test.csv', out=path + 'Data/TEST_IMAGES.pkl')\n",
    "trainLabelsOrder()\n",
    "cleanText('train.csv')\n",
    "cleanText('test.csv')\n",
    "processCleanText()\n",
    "processCleanText('clean_test.csv', 'Data/TEST_PLAINTEXT.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WewFYt7V1PK"
   },
   "source": [
    "# Train ResNet18 for Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35666,
     "status": "ok",
     "timestamp": 1607480556333,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "g6STPzZ4ocf7",
    "outputId": "5503e34c-cdff-4c3b-fc28-3314e8e24d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 60, 3) (27,)\n",
      "4315 4315\n",
      "17312 17312\n"
     ]
    }
   ],
   "source": [
    "loaded_x_train, _, loaded_y_train = loadTrainDataInOrder()\n",
    "ys = [np.where(l==1)[0][0] for l in loaded_y_train]\n",
    "cnts = Counter(ys)\n",
    "counters = [int(cnts[i]*0.2) for i in range(27)]\n",
    "x_train, y_train = [], []\n",
    "x_validate, y_validate = [], []\n",
    "for x, y in zip(loaded_x_train, loaded_y_train):\n",
    "  label = np.where(y == 1)[0][0]\n",
    "  if counters[label] > 0:\n",
    "    counters[label] -= 1\n",
    "    x_validate.append(x)\n",
    "    y_validate.append(y)\n",
    "  else:\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_validate, y_validate = np.array(x_validate), np.array(y_validate)\n",
    "print(x_train[0].shape, y_train[0].shape) # (80, 60, 3) (27,)\n",
    "print(len(x_validate), len(y_validate)) # 4315 4315\n",
    "print(len(x_train), len(y_train)) # 17312 17312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ykUv_Unro3hL"
   },
   "outputs": [],
   "source": [
    "def train_model(model, name, xtrain, ytrain, xval, yval):\n",
    "  save_best_model = ModelCheckpoint(path+name+'.h5', \n",
    "                                    monitor='val_accuracy', \n",
    "                                    mode='max', \n",
    "                                    save_best_only=True, \n",
    "                                    verbose=0)\n",
    "  datagen = ImageDataGenerator(\n",
    "      featurewise_center=False,\n",
    "      samplewise_center=False,\n",
    "      featurewise_std_normalization=False,\n",
    "      samplewise_std_normalization=False,\n",
    "      zca_whitening=False,\n",
    "      zca_epsilon=1e-06,\n",
    "      rotation_range=0,\n",
    "      width_shift_range=0,\n",
    "      height_shift_range=0,\n",
    "      shear_range=0.,\n",
    "      zoom_range=0.,\n",
    "      channel_shift_range=0.,\n",
    "      fill_mode='nearest',\n",
    "      cval=0.,\n",
    "      horizontal_flip=False,\n",
    "      vertical_flip=False,\n",
    "      rescale=None,\n",
    "      preprocessing_function=None,\n",
    "      data_format=None,\n",
    "      validation_split=0.0)\n",
    "  datagen.fit(xtrain)\n",
    "  val_data = (xval, yval)\n",
    "  if xval is None or yval is None: val_data = None\n",
    "  history = model.fit(datagen.flow(xtrain, ytrain, batch_size=batch_size),\n",
    "                      steps_per_epoch=math.ceil(xtrain.shape[0]/batch_size),\n",
    "                      epochs=epochs,\n",
    "                      validation_data=None,\n",
    "                      callbacks=[save_best_model],\n",
    "                      verbose=2)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp8MLuIvmuNe"
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "base_model = ResNet18(input_shape=(80, 60, 3), include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkrHkdBLFjkt"
   },
   "outputs": [],
   "source": [
    "# First run with val. set being 20% of each class, and then train with full training set, no val. set\n",
    "train_model(model, \"resnet18\", loaded_x_train, loaded_y_train, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhxuuj5vH9cF"
   },
   "source": [
    "# Mix Images and Text data (text using tf universal encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2405,
     "status": "ok",
     "timestamp": 1607485736041,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "72MAVv1wNu-B",
    "outputId": "241fac0d-d19f-481c-91e4-1b72880ffe75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21627, 80, 60, 3)\n",
      "21627\n",
      "(21627, 27)\n"
     ]
    }
   ],
   "source": [
    "img_data_train, text_data_train, labels_train = loadTrainDataInOrder()\n",
    "print(img_data_train.shape) #(21627, 80, 60, 3)\n",
    "print(len(text_data_train)) # 21627\n",
    "print(labels_train.shape)   #(21627, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116732,
     "status": "ok",
     "timestamp": 1607486759895,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "bL2zUiNFP9eL",
    "outputId": "203485c8-9944-4909-c941-63e87237b568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21627, 512)\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f5714ba6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f5714ba6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21627, 512)\n",
      "(21627, 1024)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "images_model = load_model(path + 'resnet18 0.9279') # test this with the other model as well\n",
    "img_extractor = Model(images_model.input, images_model.layers[-3].output)\n",
    "img_features = img_extractor.predict(img_data_train)\n",
    "print(img_features.shape) # (21627, 512)\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "text_features = embed(text_data_train).numpy()\n",
    "print(text_features.shape) # (21627, 512)\n",
    "\n",
    "train_features = np.concatenate((img_features, text_features), axis=1)\n",
    "print(train_features.shape) # (21627, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1607486779013,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "XJBeYxBhTORi",
    "outputId": "c9ff288f-4cbf-4a97-f337-5e5cfba52698"
   },
   "outputs": [],
   "source": [
    "loaded_x_train, loaded_y_train = train_features, labels_train\n",
    "\n",
    "ys = [np.where(l==1)[0][0] for l in loaded_y_train]\n",
    "cnts = Counter(ys)\n",
    "counters = [int(cnts[i]*0.2) for i in range(27)]\n",
    "x_train, y_train = [], []\n",
    "x_validate, y_validate = [], []\n",
    "for x, y in zip(loaded_x_train, loaded_y_train):\n",
    "  label = np.where(y == 1)[0][0]\n",
    "  if counters[label] > 0:\n",
    "    counters[label] -= 1\n",
    "    x_validate.append(x)\n",
    "    y_validate.append(y)\n",
    "  else:\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_validate, y_validate = np.array(x_validate), np.array(y_validate)\n",
    "print(x_train[0].shape, y_train[0].shape) # (1024,) (27,)\n",
    "print(len(x_validate), len(y_validate)) # 4315 4315\n",
    "print(len(x_train), len(y_train)) # 17312 17312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1607486783006,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "cHB-ep3OTXx9"
   },
   "outputs": [],
   "source": [
    "def train_meta_model(model, name, trainx, trainy, valx, valy):\n",
    "  # create a callback that will save the best model while training\n",
    "  save_best_model = ModelCheckpoint(path+name+'.h5', monitor='val_accuracy', \n",
    "                                    mode='max', save_best_only=True, verbose=0)\n",
    "  val_data = (valx, valy)\n",
    "  if valx is None or valy is None: val_data = None\n",
    "  history = model.fit(trainx, trainy,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=val_data,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[save_best_model],\n",
    "                      verbose=2)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1607486817630,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "UDMzo8BMUPIf"
   },
   "outputs": [],
   "source": [
    "meta_model = Sequential()\n",
    "meta_model.add(Dense(1024, input_shape=(1024,), activation='relu'))\n",
    "meta_model.add(Dense(512, activation='relu'))\n",
    "meta_model.add(Dense(256, activation='relu'))\n",
    "meta_model.add(Dense(128, activation='relu'))\n",
    "meta_model.add(Dense(num_classes, activation='softmax'))\n",
    "meta_model.compile(loss='categorical_crossentropy', \n",
    "                   optimizer=keras.optimizers.Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wS6pdvmrVMnX"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# first train with validation set, and then train with full training\n",
    "train_meta_model(meta_model, \"meta_model run5\", loaded_x_train, loaded_y_train, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4778,
     "status": "ok",
     "timestamp": 1607486859901,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "Tv_VIu4tVdJ9",
    "outputId": "a699d60a-c4a0-44a9-8ad5-90517d4886af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21628, 1024)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_data, test_text_data = loadTestData()\n",
    "test_img_features = img_extractor.predict(test_image_data)\n",
    "test_text_features = embed(test_text_data).numpy()\n",
    "test_features = np.concatenate((test_img_features, test_text_features), axis=1)\n",
    "test_features.shape # (21628, 1024) if plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3432,
     "status": "ok",
     "timestamp": 1607487175031,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "Yz8Bj58QXard",
    "outputId": "843f3d65-69db-4a97-9dc2-fea266d37964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21628\n"
     ]
    }
   ],
   "source": [
    "meta_model = load_model(path + \"meta_model run5.h5\")\n",
    "preds = meta_model.predict(test_features)\n",
    "pred_classes = [predToClass(p) for p in preds]\n",
    "print(len(pred_classes)) # 21628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1607487444176,
     "user": {
      "displayName": "Akshay Patel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjdCFEbPE1doRSfYEBHH7IxvmnKCCbI_vvJmEKkg=s64",
      "userId": "02048060651871050789"
     },
     "user_tz": 300
    },
    "id": "F5_oDWAgZ6e0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df['category'] = pred_classes\n",
    "df[['id', 'category']].to_csv('submission 4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOP/7GI62d3fsA6hP9+9K58",
   "collapsed_sections": [],
   "name": "Kaggle Comp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
